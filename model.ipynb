{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5dae3e9",
   "metadata": {},
   "source": [
    "### Phi3-4K-mini 모델불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "import torch\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "@contextmanager\n",
    "def timeout(seconds):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutError(f\"Timeout after {seconds} seconds\")\n",
    "    signal.signal(signal.SIGALRM, signal_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "\n",
    "# 캐시 디렉토리 설정\n",
    "cache_dir = os.path.join(os.getcwd(), \"model_cache\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    print(\"토크나이저 로딩 시작...\")\n",
    "    with timeout(30):  # 30초 타임아웃\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "            use_fast=False,\n",
    "            local_files_only=False,\n",
    "            trust_remote_code=True,\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "    print(\"토크나이저 로딩 완료!\")\n",
    "\n",
    "    print(\"\\n모델 로딩 시작...\")\n",
    "    with timeout(60):  # 60초 타임아웃\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            cache_dir=cache_dir,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "    print(\"모델 로딩 완료!\")\n",
    "\n",
    "    # 테스트 메시지\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n입력 처리 중...\")\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    print(\"생성 시작...\")\n",
    "    with timeout(30):  # 30초 타임아웃\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=50,  # 더 짧게 설정\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    print(\"\\n응답:\", response)\n",
    "\n",
    "except TimeoutError as te:\n",
    "    print(f\"\\n시간 초과 발생: {str(te)}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n에러 발생: {str(e)}\")\n",
    "    import traceback\n",
    "    print(\"\\n상세 에러 정보:\")\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0fc474",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"transformers>=4.45\" accelerate safetensors sentencepiece tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dfaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U transformers datasets peft accelerate bitsandbytes trl\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "\n",
    "BASE = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE)\n",
    "\n",
    "load_kwargs = dict(\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE, **load_kwargs)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora = LoraConfig(\n",
    "    r=16, lora_alpha=32, target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora)\n",
    "ds = load_dataset(\"json\", data_files={\"train\":\"train.jsonl\", \"eval\":\"eval.jsonl\"})\n",
    "def format_example(ex):\n",
    "    # Phi-3 chat 템플릿 활용해서 supervised target 만들기\n",
    "    msgs = ex[\"messages\"]\n",
    "    text = tokenizer.apply_chat_template(msgs, add_generation_prompt=False, tokenize=False)\n",
    "    return {\"text\": text}\n",
    "\n",
    "ds = ds.map(format_example, remove_columns=ds[\"train\"].column_names)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"phi3_lora_out\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    eval_strategy=\"steps\",\n",
    "    fp16=True,                               # Ampere↑에서 bf16도 가능\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"eval\"],\n",
    "    args=args,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=True,                            # 여러 샘플을 한 시퀀스로 패킹(효율↑)\n",
    "    max_seq_length=2048\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"phi3_lora_out/adapter\")\n",
    "tokenizer.save_pretrained(\"phi3_lora_out/adapter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b66fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello world!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b6d609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chldlsrb08/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "basemodel = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(basemodel)\n",
    "tokenizer = AutoTokenizer.from_pretrained(basemodel)\n",
    "\n",
    "from peft import PeftModel\n",
    "realmodel = PeftModel.from_pretrained(model, \"./phi3/checkpoint-150\")\n",
    "\n",
    "realmodel.save_pretrained(\"./phi3/final\")\n",
    "tokenizer.save_pretrained(\"./phi3/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef7aacb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/home/chldlsrb08/final_model\")\n",
    "token = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf29e59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wagyu-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
